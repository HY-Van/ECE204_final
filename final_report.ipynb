{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Academic Paper Citation Impact from Peer Review Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Project Overview\n",
    "\n",
    "This project explores the relationship between peer review evaluations and the long-term citation impact of academic papers. By analyzing reviewer comments, evaluation scores, and paper metadata, we aim to identify early indicators of scientific influence that emerge during the peer review process. Understanding these relationships could provide valuable insights for editorial decisions and improve our understanding of what constitutes impactful research.\n",
    "\n",
    "### 1.2 Problem Statement\n",
    "\n",
    "While peer review plays a crucial role in scientific publishing, its predictive power for long-term academic impact remains understudied. This project seeks to answer a fundamental question: can early evaluations during the peer review process effectively forecast citation counts, which are often used as a proxy for scientific influence? If successful, such insights could guide editorial decisions and help identify potentially high-impact research earlier in the publication pipeline.\n",
    "\n",
    "The problem is important for several reasons:\n",
    "- It could help journal editors prioritize papers with high potential impact\n",
    "- It might reveal biases or blind spots in the traditional peer review process\n",
    "- It could provide authors with insights into how reviewer feedback relates to eventual research impact\n",
    "- It contributes to a deeper understanding of how scientific influence is established and recognized\n",
    "\n",
    "### 1.3 Data Acquisition & Cleaning\n",
    "\n",
    "The dataset consists of accepted academic papers, their reviewer scores across multiple evaluation dimensions, and their citation counts. The data was initially provided in a raw format and processed through a separate preprocessing notebook. Key cleaning steps included:\n",
    "\n",
    "- Removing incomplete or inconsistent entries\n",
    "- Standardizing reviewer scores and text data\n",
    "- Filtering out papers with zero citation records\n",
    "\n",
    "The cleaned dataset was saved as `accepted_clean.pkl` and contains information on reviewer assessments across multiple categories (impact, substance, appropriateness, etc.) along with the resulting citation counts for each paper.\n",
    "\n",
    "### 1.4 Assumptions\n",
    "\n",
    "To frame our analysis, we made several key assumptions:\n",
    "- Reviewer scores are unbiased and reflective of paper quality\n",
    "- Citation counts are an appropriate proxy for scientific impact\n",
    "- All papers were published around the same time and had equal exposure opportunities\n",
    "- The relationship between review scores and citation counts is relatively stable over time\n",
    "\n",
    "### 1.5 Summary of Approach\n",
    "\n",
    "Our approach combines exploratory data analysis with predictive modeling:\n",
    "\n",
    "1. **Descriptive Analysis**: We use pivot tables, principal component analysis (PCA), and clustering to understand patterns in reviewer feedback and how they relate to citation outcomes.\n",
    "\n",
    "2. **Predictive Modeling**: We apply several supervised learning techniques to predict future citation counts, using both textual and numerical features from the peer review process. We compare multiple models and evaluate their performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Analysis\n",
    "\n",
    "Our descriptive analysis explores the relationship between reviewer assessments and citation outcomes through several analytical approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"data/accepted_clean.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes features such as reviewer scores (`novelty`, `clarity`, `impact`, `soundness`), `recommendation_level`, and final `citation_count`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by examining the distribution of citation counts in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.histplot(data=data, x='citation_count', kde=True, color=\"mediumspringgreen\" )\n",
    "ax.lines[0].set_color('darkorange')\n",
    "plt.title('Distribution of Citation Counts')\n",
    "plt.xlabel('Citation Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The citation count distribution shows a somewhat bimodal pattern with peaks around 50 and 175 citations. This suggests potential clustering of papers into different impact tiers, which we explore further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Correlation Analysis\n",
    "To understand the relationships between reviewer scores and citation counts, we examined correlations between all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "score_columns = ['IMPACT', 'SUBSTANCE', 'APPROPRIATENESS', 'MEANINGFUL_COMPARISON', \n",
    "                'SOUNDNESS_CORRECTNESS', 'ORIGINALITY', 'RECOMMENDATION', \n",
    "                'CLARITY', 'REVIEWER_CONFIDENCE', 'citation_count']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation = data[score_columns].corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='Spectral', fmt=\".2f\")\n",
    "plt.title('Correlation Between Review Scores and Citation Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the correlation analysis reveals:\n",
    "\n",
    "- Reviewer confidence has the strongest positive correlation with citation count (0.21)\n",
    "- Several metrics show weak correlations with citation outcomes\n",
    "- Some review dimensions that might intuitively seem important (like IMPACT) show surprisingly weak correlations with final citation counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Pivot Table Analysis - Average Citation by Recommendation Level\n",
    "\n",
    "We analyzed how reviewer recommendations relate to citation outcomes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 1. Pivot table analysis - average citation count by recommendation level\n",
    "pivot_recommendation = pd.pivot_table(data, \n",
    "                                     values='citation_count', \n",
    "                                     index='RECOMMENDATION', \n",
    "                                     aggfunc=['mean', 'median', 'count'])\n",
    "print(\"Average citation count by reviewer recommendation:\")\n",
    "pivot_recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot visualization further illustrates this relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize this relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='RECOMMENDATION', y='citation_count', data=data, color=\"lightskyblue\")\n",
    "plt.title('Citation Count Distribution by Reviewer Recommendation')\n",
    "plt.ylabel('Citation Count')\n",
    "plt.xlabel('Reviewer Recommendation')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the highest recommendation level (5) does not correspond to the highest citation counts. Papers with the lowest recommendation that still got published (level 1) actually show high citation counts, though the sample size is very small (2 papers). The relationship between recommendation and citation impact appears non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 PCA Analysis of Review Scores\n",
    "\n",
    "We applied Principal Component Analysis (PCA) to understand the dimensionality of reviewer assessments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 2. PCA analysis of review scores\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Select only the numeric review scores\n",
    "review_scores = data[['IMPACT', 'SUBSTANCE', 'APPROPRIATENESS', \n",
    "                      'MEANINGFUL_COMPARISON', 'SOUNDNESS_CORRECTNESS', \n",
    "                      'ORIGINALITY', 'RECOMMENDATION', 'CLARITY', \n",
    "                      'REVIEWER_CONFIDENCE']]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_scores = scaler.fit_transform(review_scores)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(scaled_scores)\n",
    "\n",
    "# Plot variance explained\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, color='mediumspringgreen')\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_), 'r-', color='crimson')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.title('PCA: Explained Variance by Component')\n",
    "plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1))\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA results show that:\n",
    "- The first two components explain approximately 43% of the variance\n",
    "- We need at least 5 components to explain 70% of the variance in review scores\n",
    "- This suggests reviewer evaluations capture multiple distinct dimensions of paper quality\n",
    "\n",
    "When we plot papers in the PCA space colored by citation count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create a DataFrame with PCA results and citation counts\n",
    "pca_df = pd.DataFrame(data=pca_result[:, 0:2], columns=['PC1', 'PC2'])\n",
    "pca_df['citation_count'] = data['citation_count']\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], \n",
    "                     c=pca_df['citation_count'], cmap='spring', \n",
    "                     alpha=0.6, s=50)\n",
    "plt.colorbar(scatter, label='Citation Count')\n",
    "plt.title('Papers in PCA Space Colored by Citation Count')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True, linestyle='--', alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization reveals no clear clustering of high-citation papers, suggesting that the relationship between review dimensions and citation impact is complex and not easily reducible to a few components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Clustering Analysis\n",
    "\n",
    "We performed K-means clustering to identify natural groupings in the reviewer assessments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 3. Clustering analysis\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Determine optimal number of clusters using the elbow method\n",
    "inertia = []\n",
    "k_range = range(15, 100)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_scores)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, marker='.', linestyle='--', color='mediumspringgreen', markerfacecolor='pink', markeredgecolor='pink', linewidth=4)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True, linestyle='--', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the elbow method, we identified k=35 as an appropriate number of clusters. When examining citation counts across these clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Apply K-means with the optimal k \n",
    "optimal_k = 35\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(scaled_scores)\n",
    "\n",
    "data_with_clusters = data.copy()\n",
    "data_with_clusters['cluster'] = cluster_labels\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='cluster', y='citation_count', data=data_with_clusters, color=\"mediumspringgreen\")\n",
    "plt.title('Citation Count Distribution by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Citation Count')\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering reveals:\n",
    "- Significant variation in citation impact across different reviewer assessment patterns\n",
    "- Some clusters (e.g., clusters 22-24) show notably higher citation rates\n",
    "- The high variance within many clusters suggests factors beyond review scores influence citation outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Analyze cluster characteristics\n",
    "cluster_profiles = data_with_clusters.groupby('cluster')[['IMPACT', 'SUBSTANCE', \n",
    "                                                        'APPROPRIATENESS', \n",
    "                                                        'MEANINGFUL_COMPARISON', \n",
    "                                                        'SOUNDNESS_CORRECTNESS', \n",
    "                                                        'ORIGINALITY', \n",
    "                                                        'RECOMMENDATION', \n",
    "                                                        'CLARITY', \n",
    "                                                        'REVIEWER_CONFIDENCE', \n",
    "                                                        'citation_count']].mean()\n",
    "print(\"Cluster profiles:\")\n",
    "cluster_profiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Insights from Descriptive Analysis\n",
    "\n",
    "Our descriptive analysis revealed several key insights:\n",
    "\n",
    "1. **Non-linear relationships**: The relationship between reviewer recommendations and citation impact is not straightforward. Papers with middling recommendations often outperform those with the highest recommendations.\n",
    "\n",
    "2. **Multidimensional quality**: PCA results suggest paper quality (as assessed by reviewers) is multidimensional and not easily reducible to one or two factors.\n",
    "\n",
    "3. **Reviewer confidence matters**: The reviewer's confidence in their assessment has one of the strongest correlations with citation impact, suggesting confident reviewers may better identify impactful work.\n",
    "\n",
    "4. **Limited predictive power**: The relatively weak correlations across most metrics suggest that review scores alone may have limited power in predicting citation impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictive Analysis\n",
    "\n",
    "### 3.1 Prediction Question\n",
    "\n",
    "Based on our descriptive analysis, we formulated the following prediction question:\n",
    "\n",
    "**Can we predict the future citation count of an academic paper based on reviewer assessments during the peer review process?**\n",
    "\n",
    "This question has practical relevance for journal editors, academic institutions, and researchers themselves, potentially helping to identify high-impact research early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['IMPACT', 'SUBSTANCE', 'APPROPRIATENESS', 'MEANINGFUL_COMPARISON', \n",
    "         'SOUNDNESS_CORRECTNESS', 'ORIGINALITY', 'RECOMMENDATION', \n",
    "         'CLARITY', 'REVIEWER_CONFIDENCE', 'PRESENTATION_FORMAT_NUM']]\n",
    "y = data['citation_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Selection and Implementation\n",
    "\n",
    "We implemented and compared multiple regression models to predict citation counts:\n",
    "\n",
    "1. Linear Regression\n",
    "2. Ridge Regression\n",
    "3. Lasso Regression\n",
    "4. Random Forest Regressor\n",
    "5. Gradient Boosting Regressor\n",
    "6. Decision Tree Regressor\n",
    "7. Support Vector Regression (SVR)\n",
    "\n",
    "Features used in the model included:\n",
    "- IMPACT\n",
    "- SUBSTANCE\n",
    "- APPROPRIATENESS\n",
    "- MEANINGFUL_COMPARISON\n",
    "- SOUNDNESS_CORRECTNESS\n",
    "- ORIGINALITY\n",
    "- RECOMMENDATION\n",
    "- CLARITY\n",
    "- REVIEWER_CONFIDENCE\n",
    "- PRESENTATION_FORMAT_NUM\n",
    "\n",
    "The target variable was citation_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Support Vector Machine': SVR()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {'RMSE': rmse, 'R2': r2, 'MAE': mae}\n",
    "    print(f\"{name} - RMSE: {rmse:.4f}, R2: {r2:.4f}, MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Performance Comparison\n",
    "\n",
    "We evaluated all models using multiple metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame(results).T\n",
    "metrics_df = metrics_df.reset_index().rename(columns={'index': 'Model'})\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='Model', y='RMSE', data=metrics_df, color=\"aquamarine\")\n",
    "plt.title('RMSE by Model')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='Model', y='R2', data=metrics_df, color=\"violet\")\n",
    "plt.title('R² by Model')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine (SVR) model performed best across multiple metrics, achieving the lowest RMSE and MAE, and the least negative R² score. However, the negative R² values across all models indicate that they perform worse than a simple mean-based prediction, suggesting significant challenges in predicting citation counts accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 SVR Model Analysis\n",
    "\n",
    "Given its relatively better performance, we further analyzed the SVR model:\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "\n",
    "We performed an extensive grid search to optimize the SVR model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "base_svr = SVR()\n",
    "base_svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate baseline model\n",
    "y_pred_base = base_svr.predict(X_test_scaled)\n",
    "rmse_base = np.sqrt(mean_squared_error(y_test, y_pred_base))\n",
    "r2_base = r2_score(y_test, y_pred_base)\n",
    "mae_base = mean_absolute_error(y_test, y_pred_base)\n",
    "\n",
    "print(f\"Baseline SVR - RMSE: {rmse_base:.4f}, R2: {r2_base:.4f}, MAE: {mae_base:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 5, 10, 25, 50, 80, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'epsilon': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=SVR(),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {-grid_search.best_score_:.4f} MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "best_svr = grid_search.best_estimator_\n",
    "\n",
    "y_pred_tuned = best_svr.predict(X_test_scaled)\n",
    "rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
    "mae_tuned = mean_absolute_error(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"Tuned SVR - RMSE: {rmse_tuned:.4f}, R2: {r2_tuned:.4f}, MAE: {mae_tuned:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "Using permutation importance, we identified which features most influenced the model predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    best_svr, X_test_scaled, y_test, \n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': result.importances_mean\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature importance ranking:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df, palette='spring')\n",
    "plt.title('Feature Importance for Citation Count Prediction (SVR Model)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis revealed:\n",
    "- ORIGINALITY and APPROPRIATENESS are the most positively influential features\n",
    "- IMPACT, surprisingly, appears to have a negative importance score\n",
    "- RECOMMENDATION has moderate positive importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Visualization\n",
    "\n",
    "We visualized the model's predictions against actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualize actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_tuned, alpha=0.5, color='mediumspringgreen')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', color='pink')\n",
    "plt.xlabel('Actual Log Citation Count')\n",
    "plt.ylabel('Predicted Log Citation Count')\n",
    "plt.title('SVR Model: Actual vs Predicted Citation Counts')\n",
    "plt.grid(True, linestyle='--', alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Calculate residuals and visualize\n",
    "residuals = y_test - y_pred_tuned\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred_tuned, residuals, alpha=0.5, color='mediumspringgreen')\n",
    "plt.axhline(y=0, color='pink', linestyle='-')\n",
    "plt.xlabel('Predicted Log Citation Count')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('SVR Model: Residual Plot')\n",
    "plt.grid(True, linestyle='--', alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualizations reveal:\n",
    "- The model struggles to predict the full range of citation counts, tending to predict values in a narrower range than actual observations\n",
    "- The residuals show heteroscedasticity, with larger errors for papers with higher citation counts\n",
    "- The model particularly underestimates citation counts for high-impact papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Predictive Analysis Insights\n",
    "\n",
    "Our predictive modeling efforts yielded several important insights:\n",
    "\n",
    "1. **Limited predictive power**: The negative R² scores across all models indicate that peer review assessments alone have limited power to predict citation outcomes accurately.\n",
    "\n",
    "2. **Originality matters**: Feature importance analysis suggests that originality assessments may be more predictive of future impact than other dimensions.\n",
    "\n",
    "3. **Non-linear relationships**: The superior performance of SVR compared to linear models suggests non-linear relationships between review scores and citation outcomes.\n",
    "\n",
    "4. **Prediction challenges**: The models struggle particularly with predicting high citation counts, which may indicate that extremely high-impact papers have qualities not fully captured in review scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ethical Considerations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Potential Biases in the Dataset\n",
    "\n",
    "Several potential sources of bias exist in this analysis:\n",
    "\n",
    "1. **Selection bias**: Our dataset includes only accepted papers, creating a truncated view that doesn't account for rejected manuscripts.\n",
    "\n",
    "2. **Citation bias**: Citation counts themselves can be influenced by factors beyond paper quality, such as author reputation, institutional prestige, or trending research areas.\n",
    "\n",
    "3. **Field-specific norms**: Different academic fields have vastly different citation patterns and norms, which our analysis doesn't account for.\n",
    "\n",
    "4. **Temporal effects**: Publications from different years have had different amounts of time to accumulate citations. Due to the limitation of the dataset we use, we only use data from ACL_2017 and CONLL_2016.\n",
    "\n",
    "### 4.2 Implications of Model Use\n",
    "\n",
    "If models like these were deployed in publishing decisions, several ethical concerns arise:\n",
    "\n",
    "1. **Self-reinforcing biases**: Using prediction models to guide editorial decisions could create feedback loops that reinforce existing biases in the publishing system.\n",
    "\n",
    "2. **Devaluing innovative research**: Truly paradigm-shifting research might initially receive mixed reviews but have long-term impact that models fail to predict.\n",
    "\n",
    "3. **Gaming the system**: Authors might learn to optimize their papers for predictive models rather than scientific contribution.\n",
    "\n",
    "4. **Disadvantaging certain groups**: If review processes contain implicit biases related to gender, institution type, or geographic location, prediction models could perpetuate these biases.\n",
    "\n",
    "### 4.3 Mitigation Strategies\n",
    "\n",
    "To address these ethical concerns, several approaches are recommended:\n",
    "\n",
    "1. **Transparency**: Any implementation of citation prediction models should be transparent about their limitations and the factors they consider.\n",
    "\n",
    "2. **Human oversight**: Prediction models should supplement, not replace, human judgment in editorial decisions.\n",
    "\n",
    "3. **Regular bias audits**: Models should be regularly audited for potential biases and updated accordingly.\n",
    "\n",
    "4. **Field normalization**: Citation predictions should be normalized by field to account for discipline-specific citation patterns. \n",
    "\n",
    "5. **Diverse metrics**: Citation impact should be considered alongside other impact measures, such as practical applications, policy influence, or educational value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "### 5.1 Summary of Findings\n",
    "\n",
    "Our analysis sought to determine whether peer review assessments can predict future citation impact of academic papers. The results indicate:\n",
    "\n",
    "1. There are statistically significant but weak relationships between review scores and citation outcomes.\n",
    "\n",
    "2. The most predictive review dimensions appear to be originality, appropriateness, and reviewer confidence.\n",
    "\n",
    "3. Models struggle to accurately predict citation counts, particularly for high-impact papers.\n",
    "\n",
    "4. The relationship between reviewer assessments and citation impact is complex, non-linear, and likely influenced by many factors beyond the scope of standard review metrics.\n",
    "\n",
    "### 5.2 Limitations\n",
    "\n",
    "Several limitations affected our analysis:\n",
    "\n",
    "1. **Data constraints**: Our dataset included only accepted papers from specific venues, limiting generalizability.\n",
    "\n",
    "2. **Limited features**: We lacked important contextual factors like author reputation, institution prestige, and topic popularity.\n",
    "\n",
    "3. **Citation count limitations**: Citations are an imperfect proxy for scientific impact, missing other forms of influence.\n",
    "\n",
    "4. **Temporal effects**: Our analysis doesn't fully account for the time papers have had to accumulate citations.\n",
    "\n",
    "5. **Model limitations**: Standard regression techniques may not fully capture the complex relationships in this domain.\n",
    "\n",
    "### 5.3 Future Directions\n",
    "\n",
    "Based on our findings, several promising directions for future research emerge:\n",
    "\n",
    "1. **Incorporating textual analysis**: Analyzing the actual text of reviewer comments could provide deeper insights than numerical scores alone.\n",
    "\n",
    "2. **Multi-modal prediction**: Combining review data with author metrics, institution information, and topic modeling could improve predictive power.\n",
    "\n",
    "3. **Longitudinal studies**: Tracking how review assessments predict citation trajectories over time could reveal temporal patterns in impact development.\n",
    "\n",
    "4. **Alternative impact metrics**: Exploring how review assessments relate to alternative impact measures (downloads, social media mentions, policy citations) could provide a more holistic view of scientific influence.\n",
    "\n",
    "5. **Causal analysis**: Methods like causal inference could help isolate the actual influence of specific review dimensions on future impact.\n",
    "\n",
    "These directions face challenges including data access limitations, the need for advanced NLP techniques, and the complex, multi-faceted nature of scientific impact. However, they offer promising paths to better understand how we can identify impactful research during the peer review process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Data Source Citation\n",
    "\n",
    "The data used in this analysis comes from the PeerRead dataset:\n",
    "\n",
    "```\n",
    "@inproceedings{kang18naacl,\n",
    "  title = {A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications},\n",
    "  author = {Dongyeop Kang and Waleed Ammar and Bhavana Dalvi and Madeleine van Zuylen and Sebastian Kohlmeier and Eduard Hovy and Roy Schwartz},\n",
    "  booktitle = {Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)},\n",
    "  address = {New Orleans, USA},\n",
    "  month = {June},\n",
    "  url = {https://arxiv.org/abs/1804.09635},\n",
    "  year = {2018}\n",
    "}\n",
    "```\n",
    "\n",
    "PeerRead is a dataset of scientific peer reviews available for research purposes. It includes reviews, rebuttals, and accept/reject decisions from several computer science venues, along with relevant paper metadata and citation counts. The dataset provides an opportunity to study the peer review process and develop applications that could potentially improve scientific publishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Statement on the Use of AI Assistance\n",
    "\n",
    "This report's structure and language were refined using GPT-based language models. The AI assistance was used to:\n",
    "\n",
    "1. Improve the organization and flow of the report sections\n",
    "2. Enhance clarity and readability through grammar and style improvements\n",
    "3. Help formulate clearer interpretations of the analytical results\n",
    "4. Ensure technical accuracy in describing machine learning approaches\n",
    "\n",
    "All data analysis, code implementation, and primary insights were developed independently prior to AI assistance. The use of AI was limited to improving the communication of results rather than generating the analytical findings themselves.\n",
    "\n",
    "Initial Prompt to GPT:\n",
    "```\n",
    "I am doing the data analysis of the review and citation, My project aims to predict the future citation impact of academic papers based on peer review text and metadata. I'll analyze how reviewer comments, evaluation scores, and paper characteristics correlate with long-term citation counts. The core problem I'm addressing is identifying early indicators of scientific impact hidden within the peer review process. I have already finish the main code and you need help me to finish checking the grammar and structure on the final report The final report notebook (final_report.ipynb) should contain the following sections in my code part\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Code Implementation Details\n",
    "\n",
    "The complete code for this analysis is available in the accompanying Jupyter notebooks:\n",
    "- `data_preprocessing.ipynb`: Contains data cleaning and preparation steps\n",
    "- `final_report.ipynb`: Contains all analysis, modeling, and visualization code presented in this report\n",
    "\n",
    "Key Python libraries used in this analysis include:\n",
    "- pandas and numpy for data manipulation\n",
    "- scikit-learn for machine learning algorithms and evaluation\n",
    "- matplotlib and seaborn for visualization\n",
    "- scipy for statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
